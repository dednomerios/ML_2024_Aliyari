# -*- coding: utf-8 -*-
"""Mini_project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s1gzqh1ekSHEWubZ6eHPxa8hsXtx4zLJ

# Question 1

## section 2
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from mpl_toolkits.mplot3d import Axes3D

X, y = make_classification(n_samples=1000, n_classes=4, n_features=3, n_clusters_per_class=1, n_redundant=0, class_sep= 0.5, random_state=54)
print(f"shape of the X is: {X.shape} and shape of the y is: {y.shape}") # X contains features and y classes
fig, axis = plt.subplots(1, 3, figsize=(14,4))
axis[0].scatter(X[:,0], X[:,1], c=y)
axis[0].set_xlabel('Feature 1')
axis[0].set_ylabel('Feature 2')
axis[0].set_title('Feature 1 vs Feature 2')
axis[1].scatter(X[:,0], X[:,2], c=y)
axis[1].set_xlabel('Feature 1')
axis[1].set_ylabel('Feature 3')
axis[1].set_title('Feature 1 vs Feature 3')
axis[2].scatter(X[:,1], X[:,2], c=y)
axis[2].set_xlabel('Feature 2')
axis[2].set_ylabel('Feature 3')
axis[2].set_title('Feature 2 vs Feature 3')

X, y = make_classification(n_samples=1000, n_classes=4, n_features=3, n_clusters_per_class=1, n_redundant=0, class_sep= 0.5, random_state=54)

fig = plt.figure(figsize=(5, 10))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y)
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')
ax.set_zlabel('Feature 3')
ax.set_title('3D Scatter Plot of Data')
plt.show()

"""## section 3"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 54)
x_train.shape, y_train.shape, x_test.shape, y_test.shape
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

LogReg_model = LogisticRegression(solver = 'sag',max_iter = 200,  random_state = 54)
LogReg_model.fit(x_train_scaled, y_train)

y_pred_logreg = LogReg_model.predict(x_test_scaled)

y_pred_logreg, y_test

accuracy_logreg = accuracy_score(y_test,y_pred_logreg)
print(f"for logestic regression the accuracy is {accuracy_logreg*100:.2f}%")

classification_rep_logreg = classification_report(y_test,y_pred_logreg)
print(classification_rep_logreg)

cm_logreg = confusion_matrix(y_test,y_pred_logreg)
sns.heatmap(cm_logreg, annot = True, fmt = 'd', cmap = 'Greens')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

LogReg_model.score(x_train_scaled, y_train)

LogReg_model.score(x_test_scaled, y_test)

SGD_model = SGDClassifier(loss = 'log', random_state = 54)
SGD_model.fit(x_train_scaled,y_train)

y_pred_SGD = SGD_model.predict(x_test_scaled)

y_pred_SGD, y_test

accuracy_SGD = accuracy_score(y_test,y_pred_SGD)
print(f"for stochastic gradient decent classifier the accurecy is {accuracy_SGD*100:.2f}%")

classification_rep_SGD = classification_report(y_test,y_pred_SGD)
print(classification_rep_SGD)

cm_SGD = confusion_matrix(y_test,y_pred_SGD)
sns.heatmap(cm_SGD, annot = True, fmt = 'd', cmap = 'Greens')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

"""## section 4"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Assuming LogReg_model is your trained Logistic Regression model
# and x_test_scaled is your scaled test dataset

# Reduce dimensions with PCA
pca = PCA(n_components=2)
x_train_pca = pca.fit_transform(x_train_scaled)
x_test_pca = pca.transform(x_test_scaled)

# Fit the model on the reduced data
LogReg_model.fit(x_train_pca, y_train)

# Create a mesh grid for the 2D plot
x_min, x_max = x_test_pca[:, 0].min() - 1, x_test_pca[:, 0].max() + 1
y_min, y_max = x_test_pca[:, 1].min() - 1, x_test_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

# Predict on the mesh grid
Z = LogReg_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(x_test_pca[:, 0], x_test_pca[:, 1], c=y_test, edgecolors='k')
plt.title('2D Decision Boundary after PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# Assuming y_pred_logreg is the predicted labels from your Logistic Regression model
misclassified = y_test != y_pred_logreg

# Plot the decision boundary
plt.contourf(xx, yy, Z, alpha=0.4)
# Scatter plot of correctly classified data points
plt.scatter(x_test_pca[~misclassified, 0], x_test_pca[~misclassified, 1], c=y_test[~misclassified], edgecolors='k', label='Correctly classified')
# Scatter plot of misclassified data points
plt.scatter(x_test_pca[misclassified, 0], x_test_pca[misclassified, 1], c=y_test[misclassified], edgecolors='k', marker='x', label='Misclassified')

plt.title('2D Decision Boundary after PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score


def plot_decision_boundaries(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    h = 0.02
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.5, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral, s=20, edgecolor='k')

    predictions = model.predict(X)
    misclassified = np.where(predictions != y)
    plt.scatter(X[misclassified, 0], X[misclassified, 1], c="white", s=50, label="Misclassified", edgecolor='k', marker='*')

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=54)

X_train_2d = X_train[:, :2]  # extracts 2 features
X_test_2d = X_test[:, :2]

log_reg_2d = LogisticRegression(max_iter=1000, C=1.0, solver='lbfgs', random_state=42)
log_reg_2d.fit(X_train_2d, y_train)

plt.figure(figsize=(7, 6))
plot_decision_boundaries(X_train_2d, y_train, log_reg_2d)

plt.show()

# plot_decision_regions(X, y, clf=LogReg_model, filler_feature_values={2: 0}, legend=2)

# # Add axis labels and title
# plt.xlabel('Feature 1 (Scaled)')
# plt.ylabel('Feature 2 (Scaled)')
# plt.title('Logistic Regression Decision Boundaries')

# plt.scatter(x_train_scaled[:, 0], x_train_scaled[:, 1], c=y_train,)

# # Show the plot
# plt.show()



# # Identify misclassified points and scatter plot them separately
# misclassified_indices = y_train != y_pred_logreg
# plt.scatter(x_train_scaled[misclassified_indices, 0], x_train_scaled[misclassified_indices, 1],
#             marker='x', s=100, c='r', label='Misclassified')

# # Show the legend
# plt.legend()

# # Show the plot
# plt.show()

"""## section 5"""

!pip install drawdata

from drawdata import ScatterWidget

widget = ScatterWidget()
widget

# Get the drawn data as a dataframe
df = widget.data_as_pandas
df

df.describe()

df.drop(columns = ['color'], inplace = True)

df

shuffled_df = df.sample(frac = 1).reset_index(drop = True)
shuffled_df

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns

X = shuffled_df[["x", "y"]]
y = shuffled_df['label']
X.shape, y.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 54, test_size = 0.2)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

model1 = LogisticRegression(solver = 'sag',max_iter = 400,  random_state = 54)
model1.fit(X_train, y_train)

y_pred = model1.predict(X_test)

accuracy_logreg = accuracy_score(y_test,y_pred)
print(f"for logestic regression the accuracy is {accuracy_logreg*100:.2f}%")

cm = confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Greens')

classification_rep = classification_report(y_test,y_pred)
print(classification_rep)

model2 = SGDClassifier(loss = 'log', random_state = 54)
model2.fit(X_train,y_train)

y_pred= model2.predict(X_test)

accuracy = accuracy_score(y_test,y_pred)
print(f"for stochastic gradient decent classifier the accurecy is {accuracy*100:.2f}%")

classification_rep = classification_report(y_test,y_pred)
print(classification_rep)

cm = confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Greens')

from sklearn.linear_model import Perceptron

model = Perceptron(penalty='l2', alpha=0.0001, max_iter=1000, tol=1e-3, random_state=54)

model.fit(X_train, y_train)
y_pred= model2.predict(X_test)
accuracy = accuracy_score(y_test,y_pred)
print(f"for Perceptron classifier the accurecy is {accuracy*100:.2f}%")

X

y

import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions
import numpy as np
from sklearn.preprocessing import LabelEncoder


label_encoder = LabelEncoder()


y_encoded = label_encoder.fit_transform(y)


X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=54, test_size=0.2)


model1 = LogisticRegression(solver='sag', max_iter=400, random_state=54)
model1.fit(X_train, y_train)


plt.figure(figsize=(10, 6))
plot_decision_regions(X.values, y_encoded, clf=model1, legend=2)


plt.xlabel('X-axis feature')
plt.ylabel('Y-axis feature')
plt.title('Decision Boundary')
plt.show()

import matplotlib.pyplot as plt


y_train_pred = model1.predict(X_train)


misclassified = y_train != y_train_pred

plt.scatter(X_train[~misclassified]['x'], X_train[~misclassified]['y'], color='green', label='Correctly classified')


plt.scatter(X_train[misclassified]['x'], X_train[misclassified]['y'], color='red', label='Misclassified')


plt.title('Visualization of Misclassified Data Points')
plt.xlabel('Feature x')
plt.ylabel('Feature y')
plt.legend()

plt.show()

"""# Question 2

## section 1
"""

import scipy.io
import pandas as pd
import numpy as np

# https://drive.google.com/file/d/1hlWSLPyd9hTrOwfGRnM8UdmBWD7ZW6_t/view?usp=sharing
!pip install --upgrade --no-cache-dir gdown
!gdown 1hlWSLPyd9hTrOwfGRnM8UdmBWD7ZW6_t

mat_data = scipy.io.loadmat('/content/99.mat')

normal_variables = ['X098_DE_time', 'X098_FE_time', 'X099_DE_time', 'X099_FE_time']

dfs = []
for i in normal_variables:
  data = mat_data[i]
  dfs.append(pd.DataFrame(data))
normal_df = pd.concat(dfs, axis=1, ignore_index=True)
normal_df.columns = normal_variables
print(normal_df)
print(f'null items: \n {normal_df.isnull().sum()}')

# https://drive.google.com/file/d/1aOgUX7ucxFx_fWxhEf6VjliR_OhQxEUj/view?usp=drive_link
!pip install --upgrade --no-cache-dir gdown
!gdown 1aOgUX7ucxFx_fWxhEf6VjliR_OhQxEUj

mat_data = scipy.io.loadmat('/content/107.mat')

fault_variables = ['X107_BA_time', 'X107_DE_time', 'X107_FE_time']
# RPM = 1748
dfs = []
for i in fault_variables:
  data = mat_data[i]
  dfs.append(pd.DataFrame(data))
fault_df = pd.concat(dfs, axis=1, ignore_index=True)
fault_df.columns = fault_variables
print(fault_df)
print(f'null items: \n {fault_df.isnull().sum()}')

"""## section 2

### section 2 a
"""

normal_data = np.array(normal_df['X099_DE_time'])
num_sample = 150
size_sample = 300
normal_matrix = np.zeros((num_sample, size_sample))

for i in range(num_sample):
  start_index = i * size_sample
  end_index = start_index + size_sample
  normal_matrix[i] = normal_data[start_index : end_index]
# normal_matrix = np.hstack((normal_matrix,label_normal))
print(normal_matrix)
normal_matrix.shape

fault_data = np.array(fault_df['X107_DE_time'])
num_sample = 150
size_sample = 300
fault_matrix = np.zeros((num_sample, size_sample))

for i in range(num_sample):
  start_index = i * size_sample
  end_index = start_index + size_sample
  fault_matrix[i] = fault_data[start_index : end_index]
# fault_matrix = np.hstack((fault_matrix,label_fault))
print(fault_matrix)
fault_matrix.shape

mix_class_data = np.vstack((normal_matrix, fault_matrix))
print(mix_class_data)
mix_class_data.shape

label_normal = np.zeros((num_sample,1))  # 0 label for NORMAL data
label_fault = np.ones((num_sample,1))    # 1 label for fault data
mix_class_labels = np.vstack((label_normal, label_fault))
mix_class_labels.shape

"""### section 2 b"""

feature_std = np.std(mix_class_data, axis=1)
print(f"Standard deviation for each row: \n {feature_std}")
len(feature_std)
# x = np.hstack((mix_class_data, feature_std.reshape(-1,1)))
# print(x)

feature_peak = np.amax(mix_class_data, axis=1)
print(f"Maximum values for each row: \n {feature_peak}")
len(feature_peak)

# xx = np.hstack((mix_class_data, feature_peak.reshape(-1,1)))
# print(xx)

feature_mean = np.mean(mix_class_data, axis=1)
print(f"Mean values for each row: \n {feature_mean}")
len(feature_mean)

# xxx = np.hstack((mix_class_data, feature_mean.reshape(-1,1)))
# print(xxx)

feature_abs_mean = np.mean(np.abs(mix_class_data), axis=1)
print(f"Absolute mean values for each row: \n {feature_abs_mean}")
len(feature_abs_mean)

# xxxx = np.hstack((mix_class_data, feature_abs_mean.reshape(-1,1)))
# print(xxxx)

features_peak_to_peak = np.ptp(mix_class_data, axis = 1)
print(f"Peak to peak values for each row: \n {features_peak_to_peak}")
len(features_peak_to_peak)
# x_5 = np.hstack((mix_class_data, features_peak_to_peak.reshape(-1,1)))
# print(x_5)

s = np.square(mix_class_data)
m = np.mean(s, axis = 1)
feature_rms = np.sqrt(m)
print(f"RMS values for each row: \n {feature_rms}")
len(feature_rms)
# x_6 = np.hstack((mix_class_data, feature_rms.reshape(-1,1)))
# print(x_6)

feature_crest_factor = feature_peak/feature_rms
print(f"Crest factor values for each row: \n {feature_crest_factor}")
len(feature_crest_factor)
# x_7 = np.hstack((mix_class_data, feature_crest_factor.reshape(-1,1)))
# print(x_7)

feature_shape_factor = feature_rms/(np.mean(np.abs(mix_class_data), axis = 1))
print(f"shape factor values for each row: \n {feature_shape_factor}")
len(feature_shape_factor)
# x_8 = np.hstack((mix_class_data, feature_shape_factor.reshape(-1,1)))
# print(x_8)

feature_list = [feature_std, feature_peak, feature_mean, feature_abs_mean, features_peak_to_peak, feature_rms, feature_crest_factor, feature_shape_factor]
len(feature_list)

mix_class_data = np.hstack((mix_class_data, mix_class_labels))
mix_class_data.shape

feature_data = np.zeros((300, 1))

for feature in feature_list:
    # Horizontally stack the current feature array with X
    feature_data = np.hstack((feature_data, feature.reshape(-1, 1)))

# Remove the first column of zeros
feature_data = feature_data[:, 1:]

print("Final feature data:")
print(feature_data)
feature_data.shape

data_with_label_features = np.hstack((mix_class_data, feature_data))
print(data_with_label_features)
data_with_label_features.shape

"""### section 3 c"""

from sklearn.utils import shuffle
shuffled_data = shuffle(data_with_label_features, random_state=54)

df = pd.DataFrame(shuffled_data)
df

X = df.iloc[:, 301:309]
y = df.iloc[:, 300]
X.columns = ['feature_std', 'feature_peak', 'feature_mean', 'feature_abs_mean', 'features_peak_to_peak', 'feature_rms', 'feature_crest_factor', 'feature_shape_factor']

X

y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 54)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""### section 2 d"""

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X_train_sc = sc.fit_transform(X_train)
X_test_sc = sc.transform(X_test)

print(X_train_sc)
X_train_sc.shape

"""## section 3"""

# import the function neeeded
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat
def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss
def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads
def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w
def accuracy(y, y_hat):
    acc = np.sum(y == np.round(y_hat)) / len(y)
    return acc

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat

w = np.random.randn(X_train_sc.shape[1],1)
y_hat = logistic_regression(X_train_sc, w)
y_hat.shape

y_train.values.reshape(-1,1).shape

def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss

bce(y_train.values.reshape(-1,1), y_hat)

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads

eta = 0.01
n_epochs = 15000

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(X_train_sc, w)

    # loss
    e = bce(y_train.values.reshape(-1,1), y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(X_train_sc, y_train.values.reshape(-1,1),y_hat)

    # gradient descent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) % 1 == 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

import matplotlib.pyplot as plt

plt.plot(error_hist)
plt.show()

accuracy(y_train.values.reshape(-1,1), y_hat)

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(X_test_sc, w)

    # loss
    e = bce(y_test.values.reshape(-1,1), y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(X_test_sc, y_test.values.reshape(-1,1),y_hat)

    # gradient descent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) % 1 == 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

plt.plot(error_hist)
plt.show()

accuracy(y_test.values.reshape(-1,1), y_hat)

X_train_b = np.hstack((np.ones((len(X_train_sc),1)), X_train_sc))
w = np.random.randn(X_train_b.shape[1],1)
error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(X_train_b, w)

    # loss
    e = bce(y_train.values.reshape(-1,1), y_hat)
    error_hist.append(e)

    # gradients
    grads = gradient(X_train_b, y_train.values.reshape(-1,1),y_hat)

    # gradient descent
    w = gradient_descent(w, eta, grads)

    if (epoch+1) % 1 == 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

plt.plot(error_hist)
plt.show()

"""## section 4"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
model = LogisticRegression()
model.fit(X_train_sc, y_train)
y_pred = model.predict(X_test_sc)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

import matplotlib.pyplot as plt
from sklearn.metrics import log_loss


model = LogisticRegression()
model.fit(X_train_sc, y_train)



y_pred_probs = model.predict_proba(X_test_sc)


loss = log_loss(y_test, y_pred_probs)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)


plt.figure(figsize=(6,4))
plt.plot([loss] * len(y_test), label='Log Loss')
plt.xlabel('Observations')
plt.ylabel('Log Loss')
plt.title('Log Loss for Each Observation')
plt.legend()
plt.show()

"""# Question 3

## section 1
"""

# https://drive.google.com/file/d/1QrvCjtSUkWrgdww92OCG_L2BNmRqZGEu/view?usp=sharing

!pip install --upgrade --no-cache-dir gdown
!gdown 1QrvCjtSUkWrgdww92OCG_L2BNmRqZGEu

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/weatherHistory.csv')
df.head()

df.info()

df.describe()

df.isnull().sum()

df.columns

df.columns = df.columns.str.replace(' ', '_')
df.head()

df.drop(['Formatted_Date', 'Loud_Cover'], axis=1, inplace=True)
df

df.dropna(axis = 0, inplace = True)
df.isnull().sum()

df["Summary"].value_counts()

plt1 = df["Summary"].value_counts(normalize = True).head(5).plot(kind = 'bar', title = 'top  weather conditoins')
plt1.set_xlabel('weather summary')
plt1.set_ylabel('percent')
plt.show()

print(f"percantage of the data with having only top 5 summary situation: {((df['Summary'].value_counts().head(5).sum())/len(df)).round(2)} %")

sns.displot(data = df, x = df["Temperature_(C)"], label = 'histogram of temprature', stat = 'density', bins = 30, color = 'r')

sns.displot(data = df, x = df["Apparent_Temperature_(C)"], label = 'histogram of apparent temprature', stat = 'density', bins = 30, color = 'g')

sns.displot(data = df, x = df["Humidity"], label = 'histogram of humidity', stat = 'density', bins = 30, color = 'skyblue')

print(f"percantage of days that Humidity is above 70% is: {(df[df.Humidity>0.7].value_counts().sum()/len(df)).round(2)} %")

sns.displot(data = df, x = df["Wind_Speed_(km/h)"], label = 'histogram of wind speed', stat = 'density', bins = 30, color = 'cyan')

print(f"percantage of days that windspeed is between 3 and 17 km/h is: {((df[df['Wind_Speed_(km/h)'].between(3,17)].value_counts().sum())/len(df)).round(2)} %")

sum_data = df.loc[(df.Summary == 'Partly Cloudy') | (df.Summary == 'Mostly Cloudy') | (df.Summary == 'Overcast') | (df.Summary == 'Clear') | (df.Summary == 'Foggy')]
sum_data = sum_data[['Summary','Temperature_(C)', 'Apparent_Temperature_(C)', 'Humidity', 'Wind_Speed_(km/h)', 'Pressure_(millibars)']]
sum_data.sample(4)

sample_size = 1000
sample = sum_data.sample(sample_size)
sns.pairplot(sample, vars=['Temperature_(C)', 'Apparent_Temperature_(C)','Humidity'], hue='Summary')
plt.show()

sample_size = 1000
sample = sum_data.sample(sample_size)
sns.pairplot(sample, vars=['Temperature_(C)', 'Apparent_Temperature_(C)', 'Humidity', 'Wind_Speed_(km/h)'], hue='Summary')
plt.show()

sample_size = 1000
sample = df.sample(sample_size)
sns.pairplot(sample, vars=['Temperature_(C)', 'Apparent_Temperature_(C)', 'Humidity', 'Wind_Speed_(km/h)', 'Pressure_(millibars)', 'Visibility_(km)'], hue='Summary')
plt.show()

df

numeric_df = df.select_dtypes(include='number')

# Calculate correlation matrix
correlation_matrix = numeric_df.corr()

# Plot correlation matrix
plt.figure(figsize=(5, 4))
sns.heatmap(correlation_matrix, annot=True, cmap='BuGn', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()

numeric_df = sum_data.select_dtypes(include='number')

correlation_matrix = numeric_df.corr()
plt.figure(figsize = (5,4))
sns.heatmap(correlation_matrix, annot = True, cmap = 'magma', fmt = '.2f')
plt.title('Correlation Matrix')
plt.show()

"""## section 2

### LS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

df.isnull().sum()

class LinearRegressionLS:
    def __init__(self):
        self.coefficients = None

    def fit(self, X, y):
        # Add a column of ones to account for the intercept term
        X = np.column_stack((np.ones(len(X)), X))

        # Compute the coefficients using the least squares method
        self.coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

    def predict(self, X):
        # Add a column of ones to account for the intercept term
        X = np.column_stack((np.ones(len(X)), X))

        # Predict the target variable
        return X.dot(self.coefficients)

X = df[['Apparent_Temperature_(C)','Temperature_(C)']]
y = df["Humidity"]

X.shape

y.shape

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 54)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

x_train.index.equals(y_train.index)

x_test.index.equals(y_test.index)

# x_train_sorted = x_train.sort_index(axis=0)
# x_train_sorted.reset_index(drop=True, inplace=True)
# print(x_train_sorted)

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

regLS_model = LinearRegressionLS()
regLS_model.fit(x_train_scaled,y_train)
y_pred = regLS_model.predict(x_test_scaled)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse.round(5))
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.grid(True)
plt.show()

"""### using 4 features

"""

X = df[['Apparent_Temperature_(C)','Temperature_(C)', 'Wind_Speed_(km/h)', 'Visibility_(km)']]
y = df["Humidity"]

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 54)
x_train.shape, y_train.shape, x_test.shape, y_test.shape

sc = StandardScaler()
x_train_sc = sc.fit_transform(x_train)
x_test_sc = sc.transform(x_test)

regLS_model = LinearRegressionLS()
regLS_model.fit(x_train_sc,y_train)
y_pred = regLS_model.predict(x_test_sc)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse.round(5))
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.grid(True)
plt.show()

"""### RLS"""

X = df[['Apparent_Temperature_(C)','Temperature_(C)']].values
y = df["Humidity"].values
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 54)

import numpy as np
import matplotlib.pyplot as plt

class RecursiveLeastSquares:
    def __init__(self, n_features, forgetting_factor=0.99):
        self.n_features = n_features
        self.forgetting_factor = forgetting_factor
        self.theta = np.zeros((n_features, 1))  # Initialize model parameters
        self.P = np.eye(n_features)  # Initialize covariance matrix

    def fit(self, X, y):
        errors = []
        for i in range(len(X)):
            x_i = X[i].reshape(-1, 1)
            y_i = y[i]

            # Predict
            y_pred = np.dot(x_i.T, self.theta)

            # Update
            error = y_i - y_pred
            errors.append(error)
            K = np.dot(self.P, x_i) / (self.forgetting_factor + np.dot(np.dot(x_i.T, self.P), x_i))
            self.theta = self.theta + np.dot(K, error)
            self.P = (1 / self.forgetting_factor) * (self.P - np.dot(K, np.dot(x_i.T, self.P)))

        return errors

    def predict(self, X):
        return np.dot(X, self.theta)

# Initialize and fit the RLS model
rls = RecursiveLeastSquares(n_features=x_train.shape[1])
errors = rls.fit(x_train, y_train)

# Make predictions
y_pred = rls.predict(x_test)

# Calculate Mean Squared Error
mse = np.mean(np.array(errors)**2)
print("Mean Squared Error:", mse.round(5))

# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.grid(True)
plt.show()

"""### using 4 features"""

X = df[['Apparent_Temperature_(C)','Temperature_(C)', 'Wind_Speed_(km/h)', 'Visibility_(km)']].values
y = df["Humidity"].values
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 54)

# Initialize and fit the RLS model
rls = RecursiveLeastSquares(n_features=x_train.shape[1])
errors = rls.fit(x_train, y_train)

# Make predictions
y_pred = rls.predict(x_test)

# Calculate Mean Squared Error
mse = np.mean(np.array(errors)**2)
print("Mean Squared Error:", mse.round(5))

# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.grid(True)
plt.show()

"""### WLS"""

X = df[['Apparent_Temperature_(C)','Temperature_(C)']]
y = df["Humidity"]

error_variance = 2  # Modify this value based on your estimation

# Calculate weights based on the estimated variance
weights = 1 / error_variance

# Fit the weighted least squares model
X_with_intercept = sm.add_constant(X)  # Add intercept term
model = sm.WLS(y, X_with_intercept, weights=weights)
result = model.fit()

# Print the model summary
print(result.summary())